---
title: "05-analysis_global"
author: "Jay Schoen"
editor_options:
  chunk_output_type: console
---

RandomForest models and visualizations of partial-dependency plots of regularly sampled (>80%) jaguars from jaguar movement database. Covariates detailed in 02-03 scripts. 

# Setup
```{r, setup, include=FALSE}
library(terra)
# library(caret)
library(ranger)
library(tidyverse)
library(tidymodels)
library(pdp)
library(plotly)
library(Hmisc)
library(vip)
library(modEvA)
```

```{r}
# knitr::opts_knit$set(root.dir = "C:/Users/jayms/Documents/Columbia E3B/PhD/Dissertation/Ch3-jaguar_movement/")

# load("~/Columbia E3B/PhD/Dissertation/Ch3-jaguar_movement/scripts/rf_analysis_ind_pt_6hr_buf_2hr_thin_mtry3_tr100_minn_400.RData")

setwd("C:/Users/jayms/Documents/Columbia E3B/PhD/Dissertation/Ch3-jaguar_movement/")
path <- "C:/Users/jayms/Documents/Columbia E3B/PhD/Dissertation/Ch3-jaguar_movement/covariate_data/"

# save progress
# save.image("~/Columbia E3B/PhD/Dissertation/Ch3-jaguar_movement/scripts/rf_analysis_ind_pt_6hr_buf_2hr_thin_mtry3_tr100_minn_400.RData")
```

## bg4 setup
```{r}
# long_lat <- "epsg:4326"
# 
# ## Stock raster 
# stock_r <- rast(str_glue("{path}/cov_stack_polygon_scaled/dens_roads_p_sc.tif")) %>%
#   `names<-`("stock_raster")
# 
# # Using polygon-standardized values with 4 bg points for each pt within 6x mean hrly step buffer
# data_all <- read_csv("covariate_data/cov_data_all_bg4_ind_pt_6hr_buf.csv") %>%
#   mutate(land_cover = as.factor(land_cover),
#          pres = as.factor(pres)) %>%
#   dplyr::select(-1)# %>%
#   # select(-c(1:2,23))
# 
# data_all_pres <- data_all %>%
#   filter(pres==1)
# 
# data_expl <- read_csv(str_glue("{path}cov_data_expl_bg4_ind_pt_6hr_buf.csv")) %>%
#   mutate(land_cover = as.factor(land_cover), 
#          pres = as.factor(pres)) %>%
#   select(-c(1:2,23))
# 
# data_ne <- read_csv(str_glue("{path}cov_data_ne_bg4_ind_pt_6hr_buf.csv")) %>%
#   mutate(land_cover = as.factor(land_cover), 
#          pres = as.factor(pres)) %>%
#   select(-c(1:2,23))
# 
# data_all_sp <- vect(data_all, geom = c("x", "y"), crs = long_lat)
# data_expl_sp <- vect(data_expl, geom = c("x", "y"), crs = long_lat)
# data_ne_sp <- vect(data_ne, geom = c("x", "y"), crs = long_lat)
# 
# covars <- names(data_all)[3:19]
# 
# 
# # 10km buffered polygons w ID
# polys_10k <- vect("jags_reg_int_ind_polygons_10k_buf.shp") %>%
#   disagg() %>%
#   `values<-`(NULL)
# polys_10k$poly_ID <- 1:length(polys_10k)


```


## bg_prop setup
```{r}
# long_lat <- "epsg:4326"
# 
# ## Stock raster 
# stock_r <- rast(str_glue("{path}/cov_stack_polygon_scaled/dens_roads_p_sc.tif")) %>%
#   `names<-`("stock_raster")
# 

# # 10km buffered polygons w ID
# polys_10k <- vect("jags_reg_int_ind_polygons_10k_buf.shp") %>%
#   disagg() %>%
#   `values<-`(NULL)
# polys_10k$poly_ID <- 1:length(polys_10k)

# Using polygon-standardized values with 1:1 random bg points for each individual within 6x mean hrly step buffer
data_all <- read_csv("covariate_data/cov_data_all_bg_rand_prop_ind_pt_6hr_buf.csv") %>%
  mutate(land_cover = as.factor(land_cover),
         pres = as.factor(pres)) %>%
  unique()                                    # removing erroneous repeats
  # dplyr::select(-c(1, 22))

# Using previously exported presences to re-add EventID and state data
pres_st <- read_csv("ind_reg_samp_st.csv")

data_all_pres <- data_all %>%
  filter(pres==1) %>%
  mutate(Event_ID = pres_st$Event_ID) %>%
  dplyr::select(Event_ID, 2:length(.))
  

# data_expl <- read_csv(str_glue("{path}cov_data_expl_bg_rand_prop_ind_pt_6hr_buf.csv")) %>%
#   mutate(land_cover = as.factor(land_cover),
#          pres = as.factor(pres))  %>%
#  dplyr::select(-c(1,22))
# 
# data_ne <- read_csv(str_glue("{path}cov_data_ne_bg_rand_prop_ind_pt_6hr_buf.csv")) %>%
#   mutate(land_cover = as.factor(land_cover),
#          pres = as.factor(pres))  %>%
#  dplyr::select(-c(1,22))
# 
# data_all_sp <- vect(data_all, geom = c("x", "y"), crs = long_lat)
# data_expl_sp <- vect(data_expl, geom = c("x", "y"), crs = long_lat)
# data_ne_sp <- vect(data_ne, geom = c("x", "y"), crs = long_lat)
# 
covars <- names(data_all)[4:20]

covars_short <- c(str_sub(covars[1:16], 1,-6), covars[17])

# # 10km buffered polygons w ID
# polys_10k <- vect("jags_reg_int_ind_polygons_10k_buf.shp") %>%
#   disagg() %>%
#   `values<-`(NULL)
# polys_10k$poly_ID <- 1:length(polys_10k)
# 
# 
# # Adding polygon ID
# data_all <- data_all %>%
#   mutate(poly_ID = terra::extract(polys_10k, data_all_sp)$poly_ID)


```

# Sampling within data (thinning)
```{r}
# ## To reduce oversampling of short interval individuals, sampling <2 hr interval individuals at 1 obs per 2 hours

# Adding individual ID's to match with EventIDs
ev_ids <- read_csv("event_id_ind_id.csv")
data_all_pres <- data_all_pres %>%
  left_join(ev_ids)

# data_all <- data_all %>%
#   left_join(ev_ids, by = "Event_ID")


# data_all_pres <- data_all %>%
#   filter(pres==1)
data_all_abs <- data_all %>%
  filter(pres==0)

# Loading interval data etc.
source("scripts/source_setup.R")
## Resetting WD
setwd("C:/Users/jayms/Documents/Columbia E3B/PhD/Dissertation/Ch3-jaguar_movement/")

ind_reg_int

# Adjusting one individual with <1 hr sampling (ind42 was 0.5 hrs but interval function rounds)
ind_reg_int[ind_reg_int$ID==42,"mod_int"] <- 0.5

# ind_state <- read_csv("ind_reg_samp_st.csv") %>%
ind_state <- pres_st %>%
  mutate(ID = as.character(ID)) %>%
  left_join(ind_reg_int %>% dplyr::select(ID, mod_int),
            by = "ID")

summary(ind_state$mod_int)
unique(ind_state$mod_int)

ind_state %>%
  group_by(mod_int) %>%
  summarise(prop = n()/nrow(.))
```

## Presence points first
```{r}
# function to sample individuals based on mod_int
## thinning high resolution animals to remove biases from their sampling

## based on id = individual ID, and t = thinning parameter (baseline interval)

smp_thin <- function(id, t) {
  d <- data_all_pres %>% filter(ID==id)
  int <- ind_reg_int %>% filter(ID==id) %>% dplyr::select(mod_int) %>% as.numeric()

  p <- round((t/int),0)    # thinning parameter (1 in every "p" steps will be kept from data)

  s <- tibble()
  if(p>1){
    s <- d[seq(1, nrow(d), p),]} else{s<-d}
          # regularly sampling data based on thinning parameter (per id)
          # if individual's mod_int >t, keeping all points the same

  return(s)
}

# Checking for lowest interval individual
smp_thin(42, 2)
data_all_pres%>%filter(ID==42)
ind_state%>%filter(ID==42)%>%dplyr::select(mod_int)

# Re-sampling to 2 hrs
## result is 30/43 ind sampled at 2 hrs, 6 at 3 hrs, 5 at 4 hrs, 1 at 5/6 hrs each
data_all_pres_smp_2hr_l <- lapply(jags_reg_int, function(x) smp_thin(x, 2)) %>%
  `names<-`(jags_reg_int)
data_all_pres_smp_2hr <- do.call(rbind, data_all_pres_smp_2hr_l) %>%
  mutate(ID = as.character(ID))

for(i in jags_reg_int) {
  assign(str_glue("jag{i}_smp_2hr_thin_all"), data_all_pres_smp_2hr_l[[i]])
}
```

## BG points
```{r}
# For BG_rand_prop
## adding ID to data_all_abs (didn't save ID on export)
abs_v <- vect("ind_polys_pts/bg_rand_prop_step_6hr_mean_buf_all.shp")

# thinning parameter =2 hrs for each individual
th_p <- data_all_pres %>%
  mutate(ID = as.character(ID)) %>%
  group_by(ID) %>%
  summarise(n_pts= n()) %>%
  left_join(ind_reg_int[,c("ID", "mod_int")]) %>%
  mutate(p = round((2/mod_int),0))   # thinning parameter (1 in every "p" steps will be kept from data)

data_all_abs <- data_all %>%
  filter(pres==0)

abs_df <- as_tibble(abs_v) %>%
  mutate(u_id = data_all_abs$u_id) %>%
  dplyr::select(u_id, 1:length(.)) %>%
  right_join(data_all_abs, by = "u_id") %>%
  dplyr::select(1, 4:length(.), 2:3)

# Checking:
identical(abs_df[,-c(23:24)], data_all_abs)

# Thinning function for absences (added "data" parameter)
## thinning based on individual's mode interval used to generate background pts
smp_thin2 <- function(data, id, t) {
  d <- data %>% filter(ID==id)
  int <- ind_reg_int %>% filter(ID==id) %>% dplyr::select(mod_int) %>% as.numeric()

  p <- round((t/int),0)    # thinning parameter (1 in every "p" steps will be kept from data)

  s <- tibble()
  if(p>1){
    s <- d[seq(1, nrow(d), p),]} else{s<-d}
          # regularly sampling data based on thinning parameter (per id)
          # if individual's mod_int >t, keeping all points the same

  return(s)
}

# Check
smp_thin2(abs_df, 13, 2)
abs_df%>%filter(ID==13)
ind_reg_int%>%filter(ID==13)


data_all_abs_smp_2hr_l <- lapply(jags_reg_int, function(x) smp_thin2(abs_df, x, 2)) %>%
  `names<-`(jags_reg_int)
data_all_abs_smp_2hr <- do.call(rbind, data_all_abs_smp_2hr_l)

# writeVector(vect(data_all_abs_smp_2hr, geom = c("x", "y")),
#             "ind_polys_pts/bg_rand_prop_2hr_thin_6hr_buf_all.shp")

data_all_smp_2hr <- bind_rows(data_all_pres_smp_2hr[,-1], data_all_abs_smp_2hr[,-1]) %>%
  mutate(u_id = 1:nrow(.)) %>%
  dplyr::select(u_id, 1:length(.))

# Expl
data_expl_smp_2hr <- data_all_pres %>%
  left_join(ind_state %>% dplyr::select(Event_ID, s, year)) %>%
  filter(Event_ID %in% data_all_pres_smp_2hr$Event_ID,
         s==3) %>%
  dplyr::select(-c(Event_ID, s)) %>%
  mutate(ID = as.character(ID)) %>%
  bind_rows(data_all_abs_smp_2hr[,-1])


# Non-expl
data_ne_smp_2hr <- data_all_pres %>%
  left_join(ind_state %>% dplyr::select(Event_ID, s, year)) %>%
  filter(Event_ID %in% data_all_pres_smp_2hr$Event_ID,
         s!=3) %>%
  dplyr::select(-c(Event_ID, s)) %>%
  mutate(ID = as.character(ID)) %>%
  bind_rows(data_all_abs_smp_2hr[,-1])


# write_csv(data_all_smp_2hr,
#           "covariate_data/cov_data_all_bg_rand_prop_2hr_thin_ind_pt_6hr_buf.csv")
# write_csv(data_expl_smp_2hr,
#           "covariate_data/cov_data_expl_bg_rand_prop_2hr_thin_ind_pt_6hr_buf.csv")
# write_csv(data_ne_smp_2hr,
#           "covariate_data/cov_data_ne_bg_rand_prop_2hr_thin_ind_pt_6hr_buf.csv")

```

# Selecting data
```{r}
# data_all <- data_all_smp_2hr
# data_expl <- data_all %>% filter(s == 3)
# data_ne <- data_all %>% filter(s != 3)

# # check
# nrow(data_all)
# nrow(data_expl) + nrow(data_ne)

data_all_smp_2hr <- read_csv("covariate_data/cov_data_all_bg_rand_prop_2hr_thin_ind_pt_6hr_buf.csv")
data_expl_smp_2hr <- read_csv("covariate_data/cov_data_expl_bg_rand_prop_2hr_thin_ind_pt_6hr_buf.csv")
data_ne_smp_2hr <- read_csv("covariate_data/cov_data_ne_bg_rand_prop_2hr_thin_ind_pt_6hr_buf.csv")

data_all <- data_all_smp_2hr %>%
  mutate(pres = as.factor(pres),
         land_cover = as.factor(land_cover)) 
data_expl <- data_expl_smp_2hr %>%
  mutate(pres = as.factor(pres),
         land_cover = as.factor(land_cover))
data_ne <- data_ne_smp_2hr %>%
  mutate(pres = as.factor(pres),
         land_cover = as.factor(land_cover))

# Export final pres/bg covariate extractions version (bg_rand_prop_2hr_thin_ind_pt_6hr_buf) as vectors for visualization

final_vects <- lapply(list(data_all_smp_2hr, data_expl_smp_2hr, data_ne_smp_2hr),
                      function(x) vect(x, geom = c('x', 'y'))) %>%
  `names<-`(c('data_all_smp_2hr', 'data_expl_smp_2hr', 'data_ne_smp_2hr'))

# for(i in 1:length(final_vects)){
#   lyr = final_vects[[i]]
#   name = names(final_vects[i])
#   writeVector(lyr, str_glue("covariate_data/{name}_bg_rand_prop_6_hr_buf.shp"))
# }

```

## For sensitivity to reg_interval (Supplementary sensitivity analysis)
```{r}
# filtering jags_reg_int for 0 hr/2 hr/4 hr buffer to test sensitivity of models to this cutoff
# Use ind_reg_int dataframe from 01-data_prep to select subset of jags within cutoff

# data_all <- data_all %>%
#   filter(ID %in% jags_reg_int)   # manipulate jags_reg_int to select different subsets
# 
# data_expl <- data_expl %>%
#   filter(ID %in% jags_reg_int)   # manipulate jags_reg_int to select different subsets
# 
# data_ne <- data_ne %>%
#   filter(ID %in% jags_reg_int)   # manipulate jags_reg_int to select different subsets

```

# EDA

```{r}
vars_grph <- c("Pop. dens.", "HII", "GPP", "%TC",
               "Dist - mod. TC", "Dist - high TC",
               "Dist - very low TC", "Dist - low TC",
               "Dist - natural", "Dist - matrix",
               "Dist - perm. water", "Dist - seas. water",
               "Dist - maj. roads", "Dens - min roads",
               "Elevation", "Slope", "Land cover")
```

## Pres/Background
### Correlation
```{r}
names(data_all)
# Correlations
c <- cor(data_all[,-c(1:3,20:24)])
c[c>0.6]
View(c)
```

### Density
```{r}
plot(density(data_all$hii_p_sc, width = 0.25), 
     xlim = c(quantile(data_all$hii_p_sc, probs = seq(0,1,0.02))[2],
              quantile(data_all$hii_p_sc, probs = seq(0,1,0.02)[50])))


# plotting everything together
par(mfrow = c(4,4), mar = c(4,3,3,1))

for(i in 4:19) {
  d <- as.data.frame(data_all)[,i]
    
    plot(density(d, width = 0.25),
         xlim = c(quantile(d, probs = seq(0,1,0.01))[2],
                  quantile(d, probs = seq(0,1,0.01))[97]),
         main = vars_grph[i-3],
         xlab = " ",
         ylab = "density",
         lwd = 2)}

# land cover
par(mfrow = c(1,1), mar = c(4,3,3,1))

ggplot(data_all, aes(x = as.factor(land_cover))) +
         geom_bar() +
  theme_bw() +
  xlab("Land Cover") +
  ylab("# Points") +
  scale_x_discrete(labels = c("Forested", "Natural non-forested", "Pasture",
                                     "Agriculture", "Mixed Ag/Pasture", "Forest plantation",
                                     "Non-vegetated", "Water", "Floodplain")) +
  theme(axis.text.x = element_text(angle = 20, vjust = 0.4))

# For density inserts (matching x axis of PDPs)
par(mfrow = c(4,4), mar = c(4,3,3,1))

plot(density(data_all$pop_dens_p_sc, width = 0.25), 
     xlim = c(0,1.5),
     xlab = " ",
     ylab = "density",
     lwd = 2)

plot(density(data_all$hii_p_sc, width = 0.25), 
     xlim = c(0,1.75),
     xlab = " ",
     ylab = "density",
     lwd = 2)

plot(density(data_all$gpp_p_sc, width = 0.25), 
     xlim = c(0,1.5),
     xlab = " ",
     ylab = "density",
     lwd = 2)

plot(density(data_all$pct_tc_p_sc, width = 0.25), 
     xlim = c(0,2.25),
     xlab = " ",
     ylab = "density",
     lwd = 2)

plot(density(data_all$dist_tc_p40_p_sc, width = 0.25), 
     xlim = c(0,0.6),
     xlab = " ",
     ylab = "density",
     lwd = 2)

plot(density(data_all$dist_non_tc_p20_p_sc, width = 0.25), 
     xlim = c(0,2.5),
     xlab = " ",
     ylab = "density",
     lwd = 2)

plot(density(data_all$dist_non_tc_p40_p_sc, width = 0.25), 
     xlim = c(0,2.75),
     xlab = " ",
     ylab = "density",
     lwd = 2)

plot(density(data_all$dist_non_nat_p_sc, width = 0.25), 
     xlim = c(0,2),
     xlab = " ",
     ylab = "density",
     lwd = 2)

plot(density(data_all$dist_wat_perm_p_sc, width = 0.25), 
     xlim = c(0,2.25),
     xlab = " ",
     ylab = "density",
     lwd = 2)

plot(density(data_all$dist_wat_seas_p_sc, width = 0.25), 
     xlim = c(0,2.25),
     xlab = " ",
     ylab = "density",
     lwd = 2)

plot(density(data_all$dist_roads_p_sc, width = 0.25), 
     xlim = c(0,2),
     xlab = " ",
     ylab = "density",
     lwd = 2)

plot(density(data_all$elev_p_sc, width = 0.25), 
     xlim = c(0.85,1.15),
     xlab = " ",
     ylab = "density",
     lwd = 2)

plot(density(data_all$slope_p_sc, width = 0.25), 
     xlim = c(0,2.25),
     xlab = " ",
     ylab = "density",
     lwd = 2)

hist(as.numeric(data_all$land_cover), 
     breaks = seq(0,9, 0.5), 
     col = "black",
     xaxt = "n")
axis(1, at = c(1:9))

```


### Variables
```{r}
# Visualizing
par(mfrow = c(1,1), mar = c(4,3,3,1))

# All pts
var_plot_all <- data_all %>%
  dplyr::select(-c(Event_ID,x,y,land_cover,poly_ID,ID)) %>%
  slice_sample(n = 1e5) %>%
  pivot_longer(cols = -pres) %>%
  ggplot(., aes(pres, value)) +
  theme_bw() +
  geom_violin(col = "grey80") +
  geom_boxplot(alpha = 0.3, col = "grey20") +
  facet_wrap(~name, scales = "free_y",
             nrow = 3, ncol = 6) + 
  labs(title = "All pts")
plot(var_plot_all)

# Exploratory pts
var_plot_expl <- data_expl %>%
  dplyr::select(-c(Event_ID,x,y,land_cover,poly_ID,ID)) %>%
  pivot_longer(cols = -pres) %>%
  ggplot(., aes(pres, value)) +
  theme_bw() +
  geom_violin(col = "grey80") +
  geom_boxplot(alpha = 0.3, col = "grey20") +
  facet_wrap(~name, scales = "free_y",
             nrow = 2, ncol = 8) +
  labs(title = "Exploratory pts")
plot(var_plot_expl)

# Non-exploratory pts
var_plot_ne <- data_ne %>%
  dplyr::select(-c(Event_ID,x,y,land_cover,poly_ID,ID)) %>%
  pivot_longer(cols = -pres) %>%
  ggplot(., aes(pres, value)) +
  theme_bw() +
  geom_violin(col = "grey80") +
  geom_boxplot(alpha = 0.3, col = "grey20") +
  facet_wrap(~name, scales = "free_y",
             nrow = 2, ncol = 8) +
  labs(title = "Non-exploratory pts")
plot(var_plot_ne)


# Land cover classes proportional to presence (0/1)
lc_plot_all <- data_all %>%
  dplyr::select(land_cover, pres) %>%
  mutate(n = 1) %>%
  group_by(pres) %>%
  mutate(total = sum(n)) %>%
  ungroup() %>%
  group_by(land_cover, pres) %>%
  dplyr::summarize(sum = sum(n),
            prop = sum/total) %>%
  ggplot(., aes(x = land_cover, y = prop, fill = pres)) +
  theme_bw() +
  geom_col(position = "dodge") +
  scale_fill_manual(values =c("grey80", "grey20")) + 
  scale_x_discrete(labels = c("Forest",
                              "Natural non-forest",
                              "Pasture",
                              "Agriculture",
                              "Mixed pasture/ag",
                              "Forest plantation",
                              "Non-vegetated",
                              "Water",
                              "Floodplain")) +
  labs(title = "Land Cover all pts",
       axis.text.x = element_text(angle = 10))
plot(lc_plot_all)

lc_plot_expl <- data_expl %>%
  dplyr::select(land_cover, pres) %>%
  mutate(n = 1) %>%
  group_by(pres) %>%
  mutate(total = sum(n)) %>%
  ungroup() %>%
  group_by(land_cover, pres) %>%
  dplyr::summarize(sum = sum(n),
            prop = sum/total) %>%
  ggplot(., aes(x = land_cover, y = prop, fill = pres)) +
  theme_bw() +
  geom_col(position = "dodge") +
  scale_fill_manual(values =c("grey80", "grey20")) + 
  scale_x_discrete(labels = c("Forest",
                              "Natural non-forest",
                              "Pasture",
                              "Agriculture",
                              "Mixed pasture/ag",
                              "Forest plantation",
                              "Non-vegetated",
                              "Water",
                              "Floodplain")) +
  labs(title = "Land Cover expl pts",
       axis.text.x = element_text(angle = 10))
plot(lc_plot_expl)

lc_plot_ne <- data_ne %>%
  select(land_cover, pres) %>%
  mutate(n = 1) %>%
  group_by(pres) %>%
  mutate(total = sum(n)) %>%
  ungroup() %>%
  group_by(land_cover, pres) %>%
  dplyr::summarize(sum = sum(n),
            prop = sum/total) %>%
  ggplot(., aes(x = land_cover, y = prop, fill = pres)) +
  theme_bw() +
  geom_col(position = "dodge") +
  scale_fill_manual(values =c("grey80", "grey20")) + 
  scale_x_discrete(labels = c("Forest",
                              "Natural non-forest",
                              "Pasture",
                              "Agriculture",
                              "Mixed pasture/ag",
                              "Forest plantation",
                              "Non-vegetated",
                              "Water",
                              "Floodplain")) +
  labs(title = "Land Cover non_expl pts",
       axis.text.x = element_text(angle = 10))
plot(lc_plot_ne)

# Combining
prop_lc_all <- data_all %>%
  select(land_cover, pres) %>%
  group_by(pres) %>%
  mutate(total = n()) %>%
  ungroup() %>%
  group_by(land_cover, pres) %>%
  dplyr::summarize(sum = n(),
            prop = sum/total) %>%
  unique() %>% mutate(model = "all")

prop_lc_exp <- data_expl %>%
  select(land_cover, pres) %>%
  group_by(pres) %>%
  mutate(total = n()) %>%
  ungroup() %>%
  group_by(land_cover, pres) %>%
  dplyr::summarize(sum = n(),
            prop = sum/total) %>%
  unique() %>% mutate(model = "expl")

prop_lc_ne <- data_ne %>%
  select(land_cover, pres) %>%
  group_by(pres) %>%
  mutate(total = n()) %>%
  ungroup() %>%
  group_by(land_cover, pres) %>%
  dplyr::summarize(sum = n(),
            prop = sum/total) %>%
  unique() %>% mutate(model = "ne")

prop_lc_comp <- bind_rows(prop_lc_all, prop_lc_exp, prop_lc_ne) %>%
  ggplot(aes(x = land_cover, y = prop, fill = model, alpha = pres)) + 
  theme_bw() +
  geom_col(position = "dodge") +
  scale_fill_manual(values =c("black", "blue", "grey70")) + 
  scale_alpha_manual(values = c(0.5, 1)) +
  scale_x_discrete(labels = c("Forest",
                              "Natural non-forest",
                              "Pasture",
                              "Agriculture",
                              "Mixed pasture/ag",
                              "Forest plantation",
                              "Non-vegetated",
                              "Water",
                              "Floodplain")) +
  xlab("Land cover category") +
  ylab("Proportion") + 
  labs(fill = "Data", alpha = "Pres")
  # labs(title = "Land Cover comparison",
  #      axis.text.x = element_text(angle = 10))
plot(prop_lc_comp)

```

## Exploratory Movement
```{r}
# d1 <- mutate(data_expl, m_state = "expl")
# d2 <- mutate(data_ne, m_state = "non-expl")

var_plot_pres <- data_all %>%
  filter(pres == 1) %>%
  dplyr::select(-c(Event_ID,x,y,land_cover,pres,poly_ID,ID)) %>%
  # slice_sample(n = 1e5) %>%
  pivot_longer(cols = -s) %>%
  ggplot(., aes(s, value)) +
  theme_bw() +
  geom_violin(col = "grey80") +
  geom_boxplot(alpha = 0.3, col = "grey20") +
  facet_wrap(~name, scales = "free_y",
             nrow = 3, ncol = 6) +
  labs("Movement state comparison")
plot(var_plot_pres)

lc_plot_pres <- data_all %>%
  filter(pres == 1) %>%
  dplyr::select(land_cover, s) %>%
  mutate(n = 1) %>%
  group_by(s) %>%
  mutate(total = sum(n)) %>%
  ungroup() %>%
  group_by(land_cover, s) %>%
  dplyr::summarize(sum = sum(n),
            prop = sum/total) %>%
  dplyr::select(-sum) %>%
  distinct() %>%
  ggplot(., aes(x = land_cover, y = prop, fill = s)) +
  theme_bw() +
  geom_col(position = "dodge") +
  scale_fill_manual(values =c("black", "blue2", "grey70")) + 
  scale_x_discrete(labels = c("Forest",
                              "Natural non-forest",
                              "Pasture",
                              "Agriculture",
                              "Mixed pasture/ag",
                              "Forest plantation",
                              "Non-vegetated",
                              "Water",
                              "Floodplain")) +
  labs(title = "Land Cover expl/ne comparison",
       axis.text.x = element_text(angle = 10))

plot(lc_plot_pres)

# differences look minimal but some expected differences
```

## Water visualization
```{r}
# Land cover analysis shows increased presence in water
## Looking at points classified as lc = water in permanent/seasonal water

wat_plot <- data_all %>%
  filter(pres==1 & land_cover %in% 8) %>%
  select(land_cover, dist_wat_perm_p_sc, dist_wat_seas_p_sc) %>%
  pivot_longer(-land_cover, values_to = "distance") %>%
  ggplot(.,aes(x = land_cover, y = distance)) +
  theme_bw() +
  geom_violin(col = "grey80", fill = "grey90") +
  geom_boxplot(alpha = 0.3, col = "grey20") + 
  facet_wrap(~name)
plot(wat_plot)

# majority of these presences are in/near seasonal water, not permanent
## remember when interpreting variable importance
```

# Smoother visualizations
```{r}
# # Building plot for each covariate with curves for all 3 models (all data, expl, and ne)
# sm_fun <- function(d1, d2, d3, cov) {
#   # d1 <- slice_sample(d1, n=5e4)
#   # d2 <- slice_sample(d2, n=5e4)
#   # d3 <- slice_sample(d3, n=5e4)
#   x1 <- as.data.frame(d1)[, cov]
#   y1 <- as.data.frame(d1)[, "pres"]
#   x2 <- as.data.frame(d2)[, cov]
#   y2 <- as.data.frame(d2)[, "pres"]
#   x3 <- as.data.frame(d3)[, cov]
#   y3 <- as.data.frame(d3)[, "pres"]
#   plsmo(x1,y1, xlab = cov, lwd = 2, trim = 0.001)  # excluding outliers (beyond 0.1%ile)
#   plsmo(x2,y2, add = T, col = "blue", lwd = 2)
#   plsmo(x3,y3, add = T, col = "grey", lwd = 2)
# }
# 
# # Single variables
# par(mfrow = c(1,1), mar = c(4,3,3,1))
# sm_fun(data_all, data_expl, data_ne, "pct_tc_p_sc")
# 
# # All variables
# par(mfrow = c(4,4), mar = c(4,3,3,1))
# lapply(covars[-17], function(x) 
#   sm_fun(data_all, data_expl, data_ne, x))
```

# Models
```{r}
set.seed(234)

### All pts
# Splitting and setting up for cross-validation
data_a_split <- initial_split(data_all, prop = 3/4)
data_a_train <- training(data_a_split)
data_a_test <- testing(data_a_split)
# data_cv <- vfold_cv(data_st_train)

set.seed(234)

### Exploratory pts
# Splitting and setting up for cross-validation
data_e_split <- initial_split(data_expl, prop = 3/4)
data_e_train <- training(data_e_split)
data_e_test <- testing(data_e_split)
# data_e_cv <- vfold_cv(data_e_train)

set.seed(234)

### Non-exploratory pts
# Splitting and setting up for cross-validation
data_ne_split <- initial_split(data_ne, prop = 3/4)
data_ne_train <- training(data_ne_split)
data_ne_test <- testing(data_ne_split)
# data_ne_cv <- vfold_cv(data_ne_train)


```


## ranger
### Univariate performance
```{r}
# Using fast fitting ranger models to assess performance of each variable independently (for feature selection)

rf_univ <- list()
univ_oob <- tibble(var = covars, oob_e = NA) 
for(i in 1:length(covars)) {
  rf_univ[[i]] <- ranger(
    formula = as.formula(str_glue("pres ~ {covars[i]}")), 
    data = data_a_train,
    num.trees = 100,
    # respect.unordered.factors = "order",
    importance = "permutation",
    probability = TRUE)
  univ_oob[i,2] <- rf_univ[[i]]$prediction.error
}
names(rf_univ) <- covars

univ_oob

rf_univ_e <- list()
univ_oob_e <- tibble(var = covars, oob_e = NA) 
for(i in 1:length(covars)) {
  rf_univ_e[[i]] <- ranger(
    formula = as.formula(str_glue("pres ~ {covars[i]}")), 
    data = data_e_train,
    num.trees = 100,
    # respect.unordered.factors = "order",
    importance = "permutation",
    probability = TRUE)
  univ_oob_e[i,2] <- rf_univ_e[[i]]$prediction.error
  names(rf_univ_e[i]) <- covars
}

names(rf_univ_e) <- covars

univ_oob_e

rf_univ_ne <- list()
univ_oob_ne <- tibble(var = covars, oob_ne = NA) 
for(i in 1:length(covars)) {
  rf_univ_ne[[i]] <- ranger(
    formula = as.formula(str_glue("pres ~ {covars[i]}")), 
    data = data_ne_train,
    num.trees = 100,
    # respect.unordered.factors = "order",
    importance = "permutation",
    probability = TRUE)
  univ_oob_ne[i,2] <- rf_univ_ne[[i]]$prediction.error
  names(rf_univ_ne[i]) <- covars
}

names(rf_univ_ne) <- covars

univ_oob_ne


```


```{r}
# Distributions of covariates
par(mfrow= c(4,4), mar = c(4,3,3,1))

for(i in covars[1:16]) {
  d <- data_all %>% dplyr::select(i) %>% unlist()
  plot(density(d), xlab = "", lwd = 2, main = str_sub(i, 1,-6))
}

par(mfrow= c(1,1), mar = c(4,3,3,1))
```


## tidymodels

### Load models
```{r}
load("model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800_final/tidy_global_all.RData")
load("model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800_final/tidy_global_expl.RData")
load("model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800_final/tidy_global_ne.RData")

# load("model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800/sens_2hr_buf_reg_int/tidy_global_all.RData")
# load("model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800/sens_2hr_buf_reg_int/tidy_global_expl.RData")
# load("model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800/sens_2hr_buf_reg_int/tidy_global_ne.RData")
# 
ranger_a_obj <- extract_fit_parsnip(final_a_model)$fit
ranger_e_obj <- extract_fit_parsnip(final_e_model)$fit
ranger_ne_obj <- extract_fit_parsnip(final_ne_model)$fit

```

### All pts

```{r}
## grouping by ID on CV and split

# for parameter tuning and clear workflow
set.seed(234)

# Splitting and setting up for cross-validation
data_a_split <- group_initial_split(data_all, group = ID, prop = 3/4)
data_a_train <- training(data_a_split)
data_a_test <- testing(data_a_split)
data_a_cv <- group_vfold_cv(data_a_train, group = ID, v = 10, prop = 3/4)

# Specifying formula and pre-processing
data_a_recipe <- recipe(pres ~ ., data = data_all[,-c(1:3,22:24)]) 

data_a_prep <- data_a_recipe %>%
  prep() %>%
  juice()

# Specify model (random forest using "ranger" algorithm)
rf_a_mod <- rand_forest() %>%
  # set_args(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_args(mtry = 3, trees = 100, min_n = tune()) %>%         # further tuning min_n
  # set_engine("ranger", importance = "permutation") %>%
  set_engine("ranger", importance = "permutation",
             sample.fraction = 0.05) %>%    # testing fraction of data used in each tree                                                   (autocorrelation control)
# permutation better to understand each variable's importance with minimal bias
  set_mode("classification")

# Workflow (put it together)
rf_a_workflow <- workflow() %>%
  add_recipe(data_a_recipe) %>%
  add_model(rf_a_mod)

# # Tune parameters
# # specify which values want to try
# rf_a_grid <- expand.grid(mtry = c(3, 4, 5),
#                        trees = c(100, 300),
#                        min_n = c(100,400,800))

# Custom metric for Boyce Index

# boyce_impl <- function(truth, estimate, case_weights = NULL) {
#   Boyce(obs = truth, pred = estimate)
# }
# 
# boyce_vec <- function(truth, estimate, na_rm = TRUE, case_weights = NULL, ...) {
#   check_class_metric(truth, estimate, case_weights)
# 
#   if (na_rm) {
#     result <- yardstick_remove_missing(truth, estimate, case_weights)
# 
#     truth <- result$truth
#     estimate <- result$estimate
#     case_weights <- result$case_weights
#   } else if (yardstick_any_missing(truth, estimate, case_weights)) {
#     return(NA_real_)
#   }
# 
#   boyce_impl(truth, estimate, case_weights = case_weights)
# }
# 
# data("solubility_test")
# 
# solubility_test$fac <- as.factor(rep(c(0,1), (nrow(solubility_test)/2)))
# boyce_impl(
#   truth = solubility_test$fac,
#   estimate = solubility_test$prediction
# )
# Boyce(
#   obs = solubility_test$solubility,
#   pred = solubility_test$prediction
# )
# #> [1] 0.5214438


# boyce_ind <- new_prob_metric(fn = boyce_vec, direction = "maximize")


# extract results
# system.time(rf_a_tune_results <- rf_a_workflow %>%
#   tune_grid(resamples = data_a_cv, #CV object
#             # grid = rf_a_grid, # grid of values to try
#             # grid = expand.grid(min_n = c(100,200,400,800,1000,1500,2000)), # testing min_n only
#             grid = expand.grid(min_n = c(400,600,800,1000)), # testing min_n only
#             metrics = metric_set(accuracy, roc_auc), # metrics we care about
#             # metrics = metric_set(boyce_ind), # can't get custom metric to work
#             control = control_resamples(save_pred = T)
#             )
# )
# 
# collect_metrics(rf_a_tune_results) %>%
#   arrange(desc(mean)) %>%
#   print(n = nrow(.))
# 
#   
# cv_metrics <- collect_metrics(rf_a_tune_results, summarize = F) 
# cv_pred <- collect_predictions(rf_a_tune_results) %>% 
#   # filter(mtry == 3 & trees == 100) %>%
#   as.data.frame()

# Boyce(obs = cv_pred$pres, pred = cv_pred$.pred_1)
# 
# Boyce(obs = cv_pred[cv_pred[,"min_n"]==50,"pres"], pred = cv_pred[cv_pred[,"min_n"]==50,".pred_1"])
# Boyce(obs = cv_pred[cv_pred[,"min_n"]==100,"pres"], pred = cv_pred[cv_pred[,"min_n"]==100,".pred_1"])
# Boyce(obs = cv_pred[cv_pred[,"min_n"]==200,"pres"], pred = cv_pred[cv_pred[,"min_n"]==200,".pred_1"])

# par(mfrow = c(3,2))
# for(i in str_pad(1:10, 2, "left", "0")) {
#   for(j in unique(cv_metrics$min_n)) {
#     sub <- cv_pred %>% filter(id == str_glue("Resample{i}"),
#                               min_n == j)
#     assign(str_glue("boyce_fold_{i}_min_n_{j}"), 
#            Boyce(obs = sub$pres, pred = sub$.pred_1, bin.width = 0.2,
#                  main = str_glue("fold_{i}_min_n_{j}")))
#   }
# }
# par(mfrow = c(1,1))
# 
# samp_grid <- expand.grid(id = c(str_glue("Resample0{1:9}"), "Resample10"), 
#                          min_n = unique(cv_metrics$min_n),
#                          .metric = "boyce",
#                          .estimator = "binary",
#                          .estimate = NA,
#                          .config = "Preprocessor1_Model1") %>%
#   as_tibble()
# 
# for(i in str_pad(1:10, 2, "left", "0")) {
#   for(j in unique(cv_metrics$min_n)){
#     b <- get(str_glue("boyce_fold_{i}_min_n_{j}"))[[2]]
#     samp_grid[samp_grid$id == str_glue("Resample{i}") & samp_grid$min_n == j, ".estimate"] = b
#   }
# }
# 
# cv_metrics_boyce <- cv_metrics %>%
#   bind_rows(samp_grid)
# 
# cv_metrics_boyce %>% 
#   group_by(min_n, .metric) %>% 
#   dplyr::summarize(mn = mean(.estimate),
#                    sd = sd(.estimate)) %>%
#   print(n=nrow(.))
# 
# cv_metrics_boyce %>% 
#   group_by(min_n, .metric) %>% 
#   dplyr::summarize(mn = mean(.estimate),
#                    sd = sd(.estimate)) %>%
#   group_by(min_n) %>%
#   dplyr::summarize(sum = sum(mn))

# param_final <- rf_tune_results %>%
#   select_best(metric = "accuracy")
param_a_final <- tibble(mtry=3, trees=100, min_n=800)  # manual assigning based on results
param_a_final

# Finalize parameters
rf_a_workflow <- rf_a_workflow %>%
  finalize_workflow(param_a_final)
rf_a_workflow

# Fit model
rf_a_fit <- rf_a_workflow %>%
  # fit on the training set and evaluate on test set
  last_fit(data_a_split)

# rf_a_fit

test_a_performance <- rf_a_fit %>% collect_metrics()
test_a_performance

# min_n = 400: acc = 0.725, roc = 0.642
# min_n = 600: acc = 0.715, roc = 0.648
# min_n = 800: acc = 0.706, roc = 0.655
# min_n = 1000: acc = 0.687, roc = 0.648
# min_n = 1200: acc = 0.699, roc = 0.657
# min_n = 1400: acc = 0.699, roc = 0.655
# min_n = 2000: acc = 0.683, roc = 0.654


# generate predictions from the test set
test_a_predictions <- rf_a_fit %>% collect_predictions()
# test_a_predictions

# Boyce index
test_a_boyce <- Boyce(obs = test_a_predictions$pres, pred = test_a_predictions$.pred_1,
                       main = str_glue("All pts_minn_{param_a_final$min_n}"))

# min_n = 400: 0.919
# min_n = 600: 0.965
# min_n = 800: 0.990
# min_n = 1000: 0.924
# min_n = 1200: 0.957
# min_n = 1400: 0.960
# min_n = 2000: 0.937


# test_a_predictions_0.55 <- test_a_predictions %>%
#   mutate(.pred_class = as.factor(ifelse(.pred_1>=0.55, 1, 0)))



# generate a confusion matrix
test_a_predictions %>%
  conf_mat(truth = pres, estimate = .pred_class) %>%
  summary(event_level = "second")

# ROC curve
test_a_predictions %>%
  roc_curve(pres, .pred_0) %>%
  ggplot(aes(1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  theme_bw()


# Final model
final_a_model <- fit(rf_a_workflow, data_all)

# variable importance
final_a_model %>%
  extract_fit_parsnip() %>%
  vip(num_features = 17)

ranger_a_obj <- extract_fit_parsnip(final_a_model)$fit
ranger_a_obj$variable.importance
```

### Expl pts
```{r}
set.seed(234)

# Splitting and setting up for cross-validation
data_e_split <- group_initial_split(data_expl, group = ID, prop = 3/4)
data_e_train <- training(data_e_split)
data_e_test <- testing(data_e_split)
data_e_cv <- group_vfold_cv(data_e_train, group = ID, prop = 3/4)

# Specifying formula and pre-processing
data_e_recipe <- recipe(pres ~ ., data = data_expl[,-c(1:2,21:23)]) 

data_e_prep <- data_e_recipe %>%
  prep() %>%
  juice()

# Specify model (random forest using "ranger" algorithm)
rf_e_mod <- rand_forest() %>%
  set_args(mtry = tune(), trees = tune(), min_n = tune()) %>%
  # set_engine("ranger", importance = "permutation") %>%
  set_engine("ranger", importance = "permutation",
             sample.fraction = 0.05) %>%    
# permutation better to understand each variable's importance with minimal bias
  set_mode("classification")

# Workflow (put it together)
rf_e_workflow <- workflow() %>%
  add_recipe(data_e_recipe) %>%
  add_model(rf_e_mod)

# Tune parameters
# specify which values eant to try
# rf_e_grid <- expand.grid(mtry = c(3, 4, 5),
#                        trees = c(100, 500))
# 
# extract results
# rf_e_tune_results <- rf_e_workflow %>%
#   tune_grid(resamples = data_e_cv, # CV object
#             grid = rf_e_grid, # grid of values to try
#             metrics = metric_set(accuracy, roc_auc) # metrics we care about
#             )
#   
# rf_e_tune_results %>%
#   collect_metrics()

# param_final <- rf_tune_results %>%
#   select_best(metric = "accuracy")
param_e_final <- tibble(mtry=3, trees=100, min_n=800)  # manual assigning based on "all" model results
param_e_final

# Finalize parameters
rf_e_workflow <- rf_e_workflow %>%
  finalize_workflow(param_e_final)

# Fit model
rf_e_fit <- rf_e_workflow %>%
  # fit on the training set and evaluate on test set
  last_fit(data_e_split)

# rf_e_fit

test_e_performance <- rf_e_fit %>% collect_metrics()
# test_e_performance

# generate predictions from the test set
test_e_predictions <- rf_e_fit %>% collect_predictions()
test_e_predictions

# Boyce index
test_e_boyce <- Boyce(obs = test_e_predictions$pres, pred = test_e_predictions$.pred_1,
                       main = str_glue("Expl pts_minn_{param_e_final$min_n}"))

# min_n = 400: acc = 0.88, roc= 0.616, boyce = 0.954
# min_n = 800: acc = 0.88, roc= 0.615, boyce = 0.977
# min_n = 1000: acc = 0.88, roc= 0.613, boyce = 0.958

# generate a confusion matrix
test_e_predictions %>% 
  conf_mat(truth = pres, estimate = .pred_class)

# ROC curve
test_e_predictions %>%
  roc_curve(pres, .pred_0) %>%
  ggplot(aes(1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  theme_bw()

# Final model
final_e_model <- fit(rf_e_workflow, data_expl)

# variable importance
final_e_model %>%
  extract_fit_parsnip() %>%
  vip(num_features = 17)

ranger_e_obj <- extract_fit_parsnip(final_e_model)$fit
ranger_e_obj$variable.importance

```

### Non-expl pts
```{r}
set.seed(234)

# Splitting and setting up for cross-validation
data_ne_split <- group_initial_split(data_ne, group = ID, prop = 3/4)
data_ne_train <- training(data_ne_split)
data_ne_test <- testing(data_ne_split)
data_ne_cv <- group_vfold_cv(data_ne_train, group = ID, prop = 3/4)

# Specifying formula and pre-processing
data_ne_recipe <- recipe(pres ~ ., data = data_ne[,-c(1:2,21:23)]) 

data_ne_prep <- data_ne_recipe %>%
  prep() %>%
  juice()

# Specify model (random forest using "ranger" algorithm)
rf_ne_mod <- rand_forest() %>%
  set_args(mtry = tune(), trees = tune(), min_n = tune()) %>%
  # set_engine("ranger", importance = "permutation") %>%
  set_engine("ranger", importance = "permutation",
             sample.fraction = 0.05) %>%    
# permutation better to understand each variable's importance with minimal bias
  set_mode("classification")

# Workflow (put it together)
rf_ne_workflow <- workflow() %>%
  add_recipe(data_ne_recipe) %>%
  add_model(rf_ne_mod)

# Tune parameters
# specify which values eant to try
# rf_ne_grid <- expand.grid(mtry = c(3, 4, 5),
#                        trees = c(100, 500))
# 
# # extract results
# rf_ne_tune_results <- rf_ne_workflow %>%
#   tune_grid(resamples = data_ne_cv, # CV object
#             grid = rf_ne_grid, # grid of values to try
#             metrics = metric_set(accuracy, roc_auc) # metrics we care about
#             )
#   
# rf_ne_tune_results %>%
#   collect_metrics()

# param_final <- rf_tune_results %>%
#   select_best(metric = "accuracy")
param_ne_final <- tibble(mtry=3, trees=100, min_n=800)  # manual assigning based on results
param_ne_final

# Finalize parameters
rf_ne_workflow <- rf_ne_workflow %>%
  finalize_workflow(param_ne_final)

# Fit model
rf_ne_fit <- rf_ne_workflow %>%
  # fit on the training set and evaluate on test set
  last_fit(data_ne_split)

test_ne_performance <- rf_ne_fit %>% collect_metrics()
test_ne_performance

# generate predictions from the test set
test_ne_predictions <- rf_ne_fit %>% collect_predictions()
test_ne_predictions

# Boyce index
test_ne_boyce <- Boyce(obs = test_ne_predictions$pres, pred = test_ne_predictions$.pred_1,
                       main = str_glue("Non-expl pts_minn_{param_ne_final$min_n}"))

# min_n = 800: acc = 0.786, roc= 0.668, boyce = 0.940
# min_n = 1000: acc = 0.787, roc= 0.673, boyce = 0.945

# generate a confusion matrix
test_ne_predictions %>% 
  conf_mat(truth = pres, estimate = .pred_class) %>%
  summary(event_level = "second")

# ROC curve
test_ne_predictions %>%
  roc_curve(pres, .pred_0) %>%
  ggplot(aes(1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  theme_bw()

# Final model
final_ne_model <- fit(rf_ne_workflow, data_ne)

# variable importance
final_ne_model %>%
  extract_fit_parsnip() %>%
  vip(num_features = 17)

ranger_ne_obj <- extract_fit_parsnip(final_ne_model)$fit
ranger_ne_obj$variable.importance

```


#### Saving models
```{r}
# save(final_a_model, file = str_glue("model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800/sens_0hr_buf_reg_int/tidy_global_all.RData"))
# save(final_e_model, file = str_glue("model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800/sens_0hr_buf_reg_int/tidy_global_expl.RData"))
# save(final_ne_model, file = str_glue("model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800/sens_0hr_buf_reg_int/tidy_global_ne.RData"))
```

## Model comparison
### Variable Importance
```{r}
var_imp <- tibble(var = covars,
                       all = ranger_a_obj$variable.importance,
                       expl = ranger_e_obj$variable.importance,
                       ne = ranger_ne_obj$variable.importance) %>%
  # need to standardize (add to 1)
  mutate(all = all/sum(all),
         expl = expl/sum(expl),
         ne = ne/sum(ne))

var_imp_plot <- var_imp %>%
  mutate(var = factor(var, levels = covars)) %>%
  pivot_longer(-var, values_to = "perm_imp", names_to = "Model") %>%
  ggplot(., aes(x = var, y = perm_imp)) +
  theme_bw() +
  geom_col(aes(fill = Model), position = "dodge") +
  scale_fill_manual(values = c("black", "blue", "grey"),
                    labels = c("All", "Exploratory", "Non-exploratory")) +
  labs(title = "Variable Importance Comparison") +
  xlab("Variable") +
  ylab("Permutation importance (relative)") +
  theme(axis.text.x = element_text(angle = 35, vjust = 1, hjust = 1)) +
  # scale_x_discrete(label = c(str_sub(covars[1:16],1,-6), "land_cover"))
  scale_x_discrete(label = vars_grph)
  
  

var_imp_plot

# ggsave("model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800/vi_comparison_samp.frac-0.25.png",
#        width = 9,
#        height = 6,
#        dpi = 350)
```

### Partial Dependence Plots
```{r}
## PDPs

partial(object = ranger_ne_obj,
        train = data_ne_train,
        pred.var = covars[17],
        which.class = 2,  # pres=1 is response
        chull = TRUE,
        prob = TRUE,
        progress = TRUE,
        trim.outliers = TRUE, 
        rug = TRUE,
        plot = TRUE)

# function to create partial dependency plots for each predictor (imported)
pd_vis_all_tidy <- lapply(covars,
                          function(x) {partial(object = ranger_a_obj,
                                               train = data_a_train,
                                               pred.var = x,
                                               which.class = 2,  # pres=1 is response
                                               chull = TRUE,
                                               prob = TRUE,
                                               progress = TRUE,
                                               trim.outliers = TRUE)
                            })

pd_vis_expl_tidy <- lapply(covars,
                           function(x) {partial(object = ranger_e_obj,
                                                train = data_e_train,
                                                pred.var = x,
                                                which.class = 2,  # pres=1 is response
                                                chull = TRUE,
                                                prob = TRUE,
                                                progress = TRUE,
                                                trim.outliers = TRUE)
                             })

pd_vis_ne_tidy <- lapply(covars,
                           function(x) {partial(object = ranger_ne_obj,
                                                train = data_ne_train,
                                                pred.var = x,
                                                which.class = 2,  # pres=1 is response
                                                chull = TRUE,
                                                prob = TRUE,
                                                progress = TRUE,
                                                trim.outliers = TRUE)
                             })

# Removing unimportant variables
pd_vis_all_tidy <- pd_vis_all_tidy[-c(5,9,14)]
pd_vis_expl_tidy <- pd_vis_expl_tidy[-c(5,9,14)]
pd_vis_ne_tidy <- pd_vis_ne_tidy[-c(5,9,14)]

# save(pd_vis_all_tidy, file = "model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800_final/pd_vis_all_tidy.RData")
# save(pd_vis_expl_tidy, file = "model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800_final/pd_vis_expl_tidy.RData")
# save(pd_vis_ne_tidy, file = "model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800_final/pd_vis_ne_tidy.RData")

load("model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800_final/pd_vis_all_tidy.RData")
load("model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800_final/pd_vis_expl_tidy.RData")
load("model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800_final/pd_vis_ne_tidy.RData")

# Comparing response curves

## adding full variable names for pdp graphics
vars_pdp <- c("Population density", "HII", "GPP", "%TC",
               "Distance - high TC", "Distance - very low TC", "Distance - low TC",
               "Distance - matrix", "Distance - perm. water", "Distance - seas. water",
               "Distance - maj. roads", "Elevation", "Slope", "Land cover")
# vars_pdp <- c("Population density", "HII", "GPP", "%TC",
#                "Distance - very low TC", "Distance - low TC",
#                "Distance - matrix", "Distance - perm. water", "Distance - seas. water",
#                "Distance - maj. roads", "Elevation", "Slope", "Land cover")

# Normalizing responses to plot together
# Scaled and centered so each curve is relative to others based on sd of pdp values
pd_vis_all_norm <- pd_vis_all_tidy
for(i in 1:length(pd_vis_all_norm)) {
  pd_vis_all_norm[[i]]$yhat <- scale(pd_vis_all_norm[[i]]$yhat) 
  colnames(pd_vis_all_norm[[i]]) <- c(vars_pdp[i], "yhat") 
}

pd_vis_expl_norm <- pd_vis_expl_tidy
for(i in 1:length(pd_vis_expl_norm)) {
  pd_vis_expl_norm[[i]]$yhat<- scale(pd_vis_expl_norm[[i]]$yhat)
  colnames(pd_vis_expl_norm[[i]]) <- c(vars_pdp[i], "yhat") 
}

pd_vis_ne_norm <- pd_vis_ne_tidy
for(i in 1:length(pd_vis_ne_norm)) {
  pd_vis_ne_norm[[i]]$yhat <- scale(pd_vis_ne_norm[[i]]$yhat)
  colnames(pd_vis_ne_norm[[i]]) <- c(vars_pdp[i], "yhat") 
}


# pd_vis_expl_norm[[5]]$yhat <- 0

# plotting everything together

## to save
# png("model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800/pdp_cov_plots_norm.png",
#      height = 1000, width = 1500, pointsize = 6, res = 220)

# svg("model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800_final/pdp_cov_plots_norm2.svg",
#     height = 10, width = 15, pointsize = 12)

par(mfrow = c(4,4), mar = c(4,3,3,1))
for(i in 1:length(pd_vis_all_norm)) {
  all_l <- pd_vis_all_norm
  expl_l <- pd_vis_expl_norm
  ne_l <- pd_vis_ne_norm
  
  # expl_l[[5]]$yhat <- pd_vis_ne_norm[[5]]$yhat    #covering up NAs
  
  if(isFALSE(is.factor(all_l[[i]][,1]))) {
    plot(all_l[[i]],
         type = "l",
         lwd = 3,
         col = "black",
         ylim = c(min(rbind(all_l[[i]]$yhat,
                            expl_l[[i]]$yhat,
                            ne_l[[i]]$yhat)),
                  max(rbind(all_l[[i]]$yhat,
                            expl_l[[i]]$yhat,
                            ne_l[[i]]$yhat))))}
  else {
    plot(pd_vis_all_norm[[i]],
         type = "p",
         pch = 95,
         cex = 4,
         col = "black",
         ylim = c(min(rbind(all_l[[i]]$yhat,
                            expl_l[[i]]$yhat,
                            ne_l[[i]]$yhat), na.rm=T),
                  max(rbind(all_l[[i]]$yhat,
                            expl_l[[i]]$yhat,
                            ne_l[[i]]$yhat), na.rm=T)))};
  if(isFALSE(is.factor(pd_vis_expl_norm[[i]][,1]))) {
    lines(pd_vis_expl_norm[[i]],
          lwd = 3,
          col = "blue")}
  else {
    points(pd_vis_expl_norm[[i]],
           pch = 95,
           cex = 4,
           col = "blue")};
  if(isFALSE(is.factor(pd_vis_ne_norm[[i]][,1]))) {
    lines(pd_vis_ne_norm[[i]],
          lwd = 3,
          col = "grey")}
  else {
    points(pd_vis_ne_norm[[i]],
           pch = 95,
           cex = 4,
           col = "grey")}
}

dev.off()

par(mfrow = c(1,1))


```

#### Interactions

##### 3d plot function
```{r}
###### Interactive 3D partial dependence plot with coloring scale ######

int_plot3d <- function(pd, var1, var2) {
  # Interpolate the partial dependence values
  dens <- akima::interp(x = get(pd)[,var1],
                      y = get(pd)[,var2],
                      z = get(pd)[,"yhat"])
  # 3D partial dependence plot with a coloring scale
  p <- plot_ly(x = dens$x, 
              y = dens$y, 
              z = dens$z,
              colors = c("red2", "yellow3", "green4", "blue3"),
              type = "surface")
  # Add axis labels for 3D plots
  # This plot seems to be switching x and y axes, so I'm correcting the labels
  # p <- p %>% layout(scene = list(xaxis = list(title = str_sub(var2, 1, -6)),
  #                                yaxis = list(title = str_sub(var1, 1, -6)),
  #                                zaxis = list(title = "Partial Dependence")))
}
```


##### Distance to non-natural areas (matrix) and pct_tc
```{r}
## All pts
pd_all_int_dist_mat_tc <- partial(object = ranger_a_obj,
            train = data_a_train,
            pred.var = c("dist_non_nat_p_sc", "pct_tc_p_sc"),
            which.class = 2,  # pres=1 is response
            chull = TRUE,
            prob = TRUE,
            progress = TRUE,
            trim.outliers = TRUE)

## Expl pts
pd_expl_int_dist_mat_tc <- partial(object = ranger_e_obj,
            train = data_e_train,
            pred.var = c("dist_non_nat_p_sc", "pct_tc_p_sc"),
            which.class = 2,  # pres=1 is response
            chull = TRUE,
            prob = TRUE,
            progress = TRUE,
            trim.outliers = TRUE)

## Non-expl pts
pd_ne_int_dist_mat_tc <- partial(object = ranger_ne_obj,
            train = data_ne_train,
            pred.var = c("dist_non_nat_p_sc", "pct_tc_p_sc"),
            which.class = 2,  # pres=1 is response
            chull = TRUE,
            prob = TRUE,
            progress = TRUE,
            trim.outliers = TRUE)

# Normalizing
# pd_all_int_dist_mat_tc$yhat <- scale(pd_all_int_dist_mat_tc$yhat)
# pd_expl_int_dist_mat_tc$yhat <- scale(pd_expl_int_dist_mat_tc$yhat)
# pd_ne_int_dist_mat_tc$yhat <- scale(pd_ne_int_dist_mat_tc$yhat)

plot_all_int_dist_mat_tc <- int_plot3d("pd_all_int_dist_mat_tc", "dist_non_nat_p_sc", "pct_tc_p_sc") %>%
  layout(scene = list(xaxis = list(title = "% Tree Cover"),
                      yaxis = list(title = "Distance to Matrix"),
                      zaxis = list(title = "Partial Dependence")))
plot_all_int_dist_mat_tc

plot_expl_int_dist_mat_tc <- int_plot3d("pd_expl_int_dist_mat_tc", "dist_non_nat_p_sc", "pct_tc_p_sc")%>%
  layout(scene = list(xaxis = list(title = "% Tree Cover"),
                      yaxis = list(title = "Distance to Matrix"),
                      zaxis = list(title = "Partial Dependence"))) %>%
  colorbar(limits = c(0.21,0.26))
plot_expl_int_dist_mat_tc

plot_ne_int_dist_mat_tc <- int_plot3d("pd_ne_int_dist_mat_tc", "dist_non_nat_p_sc", "pct_tc_p_sc") %>% 
  layout(scene = list(xaxis = list(title = "% Tree Cover"),
                      yaxis = list(title = "Distance to Matrix"),
                      zaxis = list(title = "Partial Dependence")))
plot_ne_int_dist_mat_tc


save(pd_all_int_dist_mat_tc,
     file = "model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800/pd_all_int_dist_mat_tc.RData")
save(pd_expl_int_dist_mat_tc,
     file = "model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800/pd_expl_int_dist_mat_tc.RData")
save(pd_ne_int_dist_mat_tc,
     file = "model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800/pd_ne_int_dist_mat_tc.RData")

load("model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800/pd_all_int_dist_mat_tc.RData")
load("model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800/pd_expl_int_dist_mat_tc.RData")
load("model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800/pd_ne_int_dist_mat_tc.RData")


```

##### % Tree Cover & Distance to TC
```{r}
## All pts
pd_all_int_pct_tc_dist_tc_p40 <- partial(object = ranger_a_obj,
            train = data_a_train,
            pred.var = c("pct_tc_p_sc", "dist_tc_p40_p_sc"),
            which.class = 2,  # pres=1 is response
            chull = TRUE,
            prob = TRUE,
            progress = TRUE,
            trim.outliers = TRUE)

## Expl pts
pd_expl_int_pct_tc_dist_tc_p40 <- partial(object = ranger_e_obj,
            train = data_e_train,
            pred.var = c("pct_tc_p_sc", "dist_tc_p40_p_sc"),
            which.class = 2,  # pres=1 is response
            chull = TRUE,
            prob = TRUE,
            progress = TRUE,
            trim.outliers = TRUE)

## Non-expl pts
pd_ne_int_pct_tc_dist_tc_p40 <- partial(object = ranger_ne_obj,
            train = data_ne_train,
            pred.var = c("pct_tc_p_sc", "dist_tc_p40_p_sc"),
            which.class = 2,  # pres=1 is response
            chull = TRUE,
            prob = TRUE,
            progress = TRUE,
            trim.outliers = TRUE)


plot_all_int_pct_tc_dist_tc_p40 <- int_plot3d("pd_all_int_pct_tc_dist_tc_p40", "pct_tc_p_sc", "dist_tc_p40_p_sc") %>% 
  layout(scene = list(xaxis = list(title = "Distance to Tree Cover (40th %ile)"),
                      yaxis = list(title = "% Tree Cover"),
                      zaxis = list(title = "Partial Dependence")))
plot_all_int_pct_tc_dist_tc_p40

plot_expl_int_pct_tc_dist_tc_p40 <- int_plot3d("pd_expl_int_pct_tc_dist_tc_p40", "pct_tc_p_sc", "dist_tc_p40_p_sc")%>% 
  layout(scene = list(xaxis = list(title = "Distance to Tree Cover (40th %ile)"),
                      yaxis = list(title = "% Tree Cover"),
                      zaxis = list(title = "Partial Dependence")))
plot_expl_int_pct_tc_dist_tc_p40

plot_ne_int_pct_tc_dist_tc_p40 <- int_plot3d("pd_ne_int_pct_tc_dist_tc_p40", "pct_tc_p_sc", "dist_tc_p40_p_sc") %>% 
  layout(scene = list(xaxis = list(title = "Distance to Tree Cover (40th %ile)"),
                      yaxis = list(title = "% Tree Cover"),
                      zaxis = list(title = "Partial Dependence")))
plot_ne_int_pct_tc_dist_tc_p40

save(pd_all_int_pct_tc_dist_tc_p40,
     file = "model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800/pd_all_int_pct_tc_dist_tc_p40.RData")
save(pd_expl_int_pct_tc_dist_tc_p40, 
     file = "model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800/pd_expl_int_pct_tc_dist_tc_p40.RData")
save(pd_ne_int_pct_tc_dist_tc_p40, 
     file = "model_global/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800/pd_ne_int_pct_tc_dist_tc_p40.RData")
```

##### Distance to roads and pct_tc
```{r}
## All pts
pd_all_int_dist_rd_tc <- partial(object = ranger_a_obj,
            train = data_a_train,
            pred.var = c("dist_roads_p_sc", "pct_tc_p_sc"),
            which.class = 2,  # pres=1 is response
            chull = TRUE,
            prob = TRUE,
            progress = TRUE,
            trim.outliers = TRUE)

## Expl pts
pd_expl_int_dist_rd_tc <- partial(object = ranger_e_obj,
            train = data_e_train,
            pred.var = c("dist_roads_p_sc", "pct_tc_p_sc"),
            which.class = 2,  # pres=1 is response
            chull = TRUE,
            prob = TRUE,
            progress = TRUE,
            trim.outliers = TRUE)

## Non-expl pts
pd_ne_int_dist_rd_tc <- partial(object = ranger_ne_obj,
            train = data_ne_train,
            pred.var = c("dist_roads_p_sc", "pct_tc_p_sc"),
            which.class = 2,  # pres=1 is response
            chull = TRUE,
            prob = TRUE,
            progress = TRUE,
            trim.outliers = TRUE)

plot_all_int_dist_rd_tc <- int_plot3d("pd_all_int_dist_rd_tc", "dist_roads_p_sc", "pct_tc_p_sc") %>% 
  layout(scene = list(xaxis = list(title = "% Tree Cover"),
                      yaxis = list(title = "Distance to Roads"),
                      zaxis = list(title = "Partial Dependence")))
plot_all_int_dist_rd_tc

plot_expl_int_dist_rd_tc <- int_plot3d("pd_expl_int_dist_rd_tc", "dist_roads_p_sc", "pct_tc_p_sc") %>%
  layout(scene = list(xaxis = list(title = "% Tree Cover"),
                      yaxis = list(title = "Distance to Roads"),
                      zaxis = list(title = "Partial Dependence")))
plot_expl_int_dist_rd_tc

plot_ne_int_dist_rd_tc <- int_plot3d("pd_ne_int_dist_rd_tc", "dist_roads_p_sc", "pct_tc_p_sc") %>% 
  layout(scene = list(xaxis = list(title = "% Tree Cover"),
                      yaxis = list(title = "Distance to Roads"),
                      zaxis = list(title = "Partial Dependence")))
plot_ne_int_dist_rd_tc

# save(pd_all_int_dist_rd_tc, file = "model_global/bg_rand_prop/pd_all_int_dist_rd_tc.RData")
# save(pd_expl_int_dist_rd_tc, file = "model_global/bg_rand_prop/pd_expl_int_dist_rd_tc.RData")
# save(pd_ne_int_dist_rd_tc, file = "model_global/bg_rand_prop/pd_ne_int_dist_rd_tc.RData")


```

##### Non-tc p20 & Seasonal water
```{r}
## All pts
pd_all_int_dist_non_tc_p20_wat_seas <- partial(object = ranger_a_obj,
            train = data_a_train,
            pred.var = c("dist_non_tc_p20_p_sc", "dist_wat_seas_p_sc"),
            which.class = 2,  # pres=1 is response
            chull = TRUE,
            prob = TRUE,
            progress = TRUE,
            trim.outliers = TRUE)

## Expl pts
pd_expl_int_dist_non_tc_p20_wat_seas <- partial(object = ranger_e_obj,
            train = data_e_train,
            pred.var = c("dist_non_tc_p20_p_sc", "dist_wat_seas_p_sc"),
            which.class = 2,  # pres=1 is response
            chull = TRUE,
            prob = TRUE,
            progress = TRUE,
            trim.outliers = TRUE)

## Non-expl pts
pd_ne_int_dist_non_tc_p20_wat_seas <- partial(object = ranger_ne_obj,
            train = data_ne_train,
            pred.var = c("dist_non_tc_p20_p_sc", "dist_wat_seas_p_sc"),
            which.class = 2,  # pres=1 is response
            chull = TRUE,
            prob = TRUE,
            progress = TRUE,
            trim.outliers = TRUE)

plot_all_int_dist_non_tc_p20_wat_seas <- int_plot3d("pd_all_int_dist_non_tc_p20_wat_seas", "dist_non_tc_p20_p_sc", "dist_wat_seas_p_sc") %>% 
  layout(scene = list(xaxis = list(title = "Distance to Water (seasonal)"),
                      yaxis = list(title = "Distance to Low Tree Cover (20th %ile)"),
                      zaxis = list(title = "Partial Dependence")))
plot_all_int_dist_non_tc_p20_wat_seas

plot_expl_int_dist_non_tc_p20_wat_seas <- int_plot3d("pd_expl_int_dist_non_tc_p20_wat_seas", "dist_non_tc_p20_p_sc", "dist_wat_seas_p_sc") %>% 
  layout(scene = list(xaxis = list(title = "Distance to Water (seasonal)"),
                      yaxis = list(title = "Distance to Low Tree Cover (20th %ile)"),
                      zaxis = list(title = "Partial Dependence")))
plot_expl_int_dist_non_tc_p20_wat_seas

plot_ne_int_dist_non_tc_p20_wat_seas <- int_plot3d("pd_ne_int_dist_non_tc_p20_wat_seas", "dist_non_tc_p20_p_sc", "dist_wat_seas_p_sc") %>% 
  layout(scene = list(xaxis = list(title = "Distance to Water (seasonal)"),
                      yaxis = list(title = "Distance to Low Tree Cover (20th %ile)"),
                      zaxis = list(title = "Partial Dependence")))
plot_ne_int_dist_non_tc_p20_wat_seas

```



# Predict
## Splitting polygons for predictions
```{r}
# 10k polys
polys_10k <- polys_10k %>%
  disagg() %>%
  `values<-`(NULL)
polys_10k$poly_ID <- 1:length(polys_10k)

for(i in 1:length(polys_10k)) {
  assign(str_glue("poly{i}"), subset(polys_10k, polys_10k$poly_ID==i))
}

# # Indiviudal polygons from 6hr mean step length buffer on each point
# ind_polys_all <- vect("ind_polys_pts/polys_step_6hr_mean_buf_all.shp") %>%
#   aggregate() %>%
#   disagg()
# ind_polys_expl <- vect("ind_polys_pts/polys_step_6hr_mean_buf_expl.shp") %>%
#   aggregate() %>%
#   disagg()
# ind_polys_ne <- vect("ind_polys_pts/polys_step_6hr_mean_buf_ne.shp") %>%
#   aggregate() %>%
#   disagg()
# 
# for(i in ind_polys_all$ID) {
#   assign(str_glue("jag{i}_poly_all"), subset(ind_polys_all, ind_polys_all$ID==i))
#   assign(str_glue("jag{i}_poly_expl"), subset(ind_polys_expl, ind_polys_expl$ID==i))
#   assign(str_glue("jag{i}_poly_ne"), subset(ind_polys_ne, ind_polys_ne$ID==i))
# }

# poly8 <- subset(polys_10k, polys_10k$poly_ID==8)
# plot(poly8)
```

### Function for poly prediction
```{r}
## Polygon scaled rasters
stack_covar_p_sc <- list.files("covariate_data/cov_stack_polygon_scaled/", 
                               pattern = ".tif",
                               all.files = TRUE,
                               full.names = TRUE) %>%
  rast()

# covars_short <- c(str_sub(covars[1:16], 1,-6), covars[17])
for(i in covars_short[1:16]) {
  assign(str_glue("{i}_p_sc"), c(stack_covar_p_sc[i]))
}

lc <- list.files("covariate_data/landcover/250m/",
                 pattern = "lc_2",
                 all.files = TRUE, 
                 full.names = TRUE) %>%
  rast() %>%
  `names<-`(str_glue("lc_{2000:2016}"))

# pr_lc_2014 <- project(lc[[15]], stock_r, method = "near")

predict_stack <- c(dist_roads_p_sc, dens_roads_p_sc, slope_p_sc, elev_p_sc,
                   pop_dens_p_sc[[15]], hii_p_sc[[15]], gpp_p_sc[[15]],
                   pct_tc_p_sc[[15]], dist_tc_p20_p_sc[[15]], dist_tc_p40_p_sc[[15]],
                   dist_non_tc_p20_p_sc[[15]], dist_non_tc_p40_p_sc[[15]],
                   dist_nat_p_sc[[15]], dist_non_nat_p_sc[[15]],
                   dist_wat_perm_p_sc[[15]],dist_wat_seas_p_sc[[15]],
                   lc[[15]])

names(predict_stack)[5:16] <- str_sub(names(predict_stack)[5:16],1,-11)
names(predict_stack)[5:16] <- str_glue("{names(predict_stack)[5:16]}_p_sc")
names(predict_stack)[17] <- "land_cover"
names(predict_stack)

pr_poly <- function(poly, mod) {
  p <- get(str_glue("poly{poly}"))
  s <- predict_stack %>% crop(p) %>% terra::mask(p)
  lc <- s[[17]]
  lc[lc==0] <- NA
  # f <- c(scale(s[[1:16]], center=F), lc)
  f <- c(s[[1:16]], lc)
  f[is.na(f)] <- 7                          # set NA areas to rare lc/value
  
  prd <- terra::predict(f, mod, factors = "land_cover", na.rm = T)
  prd_c <- prd[[2]] %>% crop(p) %>% terra::mask(p)      # 2nd layer is prob of pres (1), not 0
  
  # # inverting result (default seems to be prob of pres = 0, not 1)
  # prd_inv <- app(prd_c, function(x) 1-x)  
  
  return(prd_c)
}
```

### Poly test
```{r}
par(mfrow = c(1,1), mar = c(3,4,4,1))
pr_poly3_a <- pr_poly(3, ranger_a_obj)
plot(pr_poly3_a)
pr_poly3_e <- pr_poly(3, ranger_e_obj)
plot(pr_poly3_e)
pr_poly3_ne <- pr_poly(3, ranger_ne_obj)
plot(pr_poly3_ne)


# plot(data_e_sp, add = T, cex = 0.3)
```

### Poly predict
```{r}
pr_polys_all <- lapply(1:length(polys_10k), function(x) pr_poly(x, ranger_a_obj))
pr_polys_expl <- lapply(1:length(polys_10k), function(x) pr_poly(x, ranger_e_obj))
pr_polys_ne <- lapply(1:length(polys_10k), function(x) pr_poly(x, ranger_ne_obj))

# Exporting "all" projection tests
# for (i in 1:length(pr_polys_all)) {
#   export <- pr_polys_all[[i]]
#   filename <- str_glue("polys_yr_projections/tests/smp_2hr_bg1_rand_group_id/poly{i}_2014_samp_2hr_bg1_rand_6hr_buf_id_group.tif")
#   writeRaster(export, filename)
# }

par(mfrow = c(4,3), mar = c(3,4,4,1))

for(i in c(1,3,6,9)) {
  plot(pr_polys_all[[i]], main = "all")
  plot(pr_polys_expl[[i]], main = "expl")
  plot(pr_polys_ne[[i]], main = "ne")
}

# pr_all <- do.call(merge, pr_polys_all)
# pr_expl <- do.call(merge, pr_polys_expl)
# pr_ne <- do.call(merge, pr_polys_ne)
# 
# # Comparing means
# lapply(c(pr_all, pr_expl, pr_ne), values) %>% lapply(function(x) mean(x, na.rm=T))
# 
# pr_stack <- c(pr_all, pr_expl, pr_ne) %>%
#   `names<-`(c("all", "expl", "ne"))
# 
# writeRaster(pr_stack, c("model_global/pr_all_6hr_buf_inv.tif",
#                         "model_global/pr_expl_6hr_buf_inv.tif",
#                         "model_global/pr_ne_6hr_buf_inv.tif"))

# for (i in 1:length(pr_polys_all)) {
#   export <- pr_polys_all[[i]]
#   filename <- str_glue("polys_yr_projections/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800_all/poly{i}_2014_samp_2hr_bg1_rand_6hr_buf_id_group.tif")
#   writeRaster(export, filename)
# }
# 
# for (i in 1:length(pr_polys_expl)) {
#   export <- pr_polys_expl[[i]]
#   filename <- str_glue("polys_yr_projections/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800_expl/poly{i}_2014_samp_2hr_bg1_rand_6hr_buf_id_group.tif")
#   writeRaster(export, filename)
# }
# 
# for (i in 1:length(pr_polys_ne)) {
#   export <- pr_polys_ne[[i]]
#   filename <- str_glue("polys_yr_projections/bg_rand_prop_2hr_thin_mtry3_tr100_minn_800_ne/poly{i}_2014_samp_2hr_bg1_rand_6hr_buf_id_group.tif")
#   writeRaster(export, filename)
# }

```
